{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Base directories\n",
    "source_base_dir = \"xml\"\n",
    "target_base_dir = \"txt\"\n",
    "\n",
    "# Modified convert_filename function to include count\n",
    "def convert_filename(filename, count):\n",
    "    date_part = filename.split('_')[0]  # Extract date part\n",
    "    date_obj = datetime.strptime(date_part, '%Y%m%d')\n",
    "    # Include the day of the week and prepend count in the new filename\n",
    "    new_filename = f\"{count}-{date_obj.strftime('%A, %d %B, %Y')}.txt\"\n",
    "    return new_filename\n",
    "\n",
    "# Modified process_files function to track file count per year\n",
    "def process_files(subdir):\n",
    "    source_dir = os.path.join(source_base_dir, subdir)\n",
    "    target_dir = os.path.join(target_base_dir, subdir)\n",
    "    os.makedirs(target_dir, exist_ok=True)  # Ensure target directory exists\n",
    "\n",
    "    yearly_count = {}  # Dictionary to keep track of file counts per year\n",
    "\n",
    "    for filename in sorted(os.listdir(source_dir)):  # Sort to ensure chronological order\n",
    "        if filename.endswith('.xml'):\n",
    "            year = filename.split('_')[0][:4]  # Extract year part from filename\n",
    "            yearly_count[year] = yearly_count.get(year, 0) + 1  # Increment count for the year\n",
    "\n",
    "            new_filename = convert_filename(filename, yearly_count[year])\n",
    "            source_file_path = os.path.join(source_dir, filename)\n",
    "            target_file_path = os.path.join(target_dir, new_filename)\n",
    "\n",
    "            # Read XML and save content to TXT\n",
    "            tree = ET.parse(source_file_path)\n",
    "            root = tree.getroot()\n",
    "            with open(target_file_path, 'w') as txt_file:\n",
    "                txt_file.write(ET.tostring(root, encoding='unicode'))\n",
    "\n",
    "# Process files for both 'hofreps' and 'senate'\n",
    "process_files('hofreps')\n",
    "process_files('senate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os  # Ensure os is imported for directory walking\n",
    "\n",
    "# Directories to process\n",
    "directories = ['txt/hofreps', 'txt/senate']\n",
    "\n",
    "def clean_and_deduplicate_pages(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    content = content.replace('<page.no>', '<page>').replace('</page.no>', '</page>')\n",
    "    pages = re.findall(r'(<page>(.*?)</page>)', content)\n",
    "    seen = {}\n",
    "    cleaned_content = content\n",
    "    for match in reversed(pages):\n",
    "        full_tag, page_content = match\n",
    "        if page_content in seen:\n",
    "            cleaned_content = cleaned_content.replace(full_tag, '', 1)\n",
    "        else:\n",
    "            seen[page_content] = True\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "\n",
    "def remove_tags(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    content = re.sub(r'<hansard xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"../../hansard.xsd\" version=\"2.1\">', '', content)\n",
    "    lines = content.splitlines()\n",
    "    modified_content = []\n",
    "    for line in lines:\n",
    "        if not re.search(r'(<proof>.*?</proof>|<day\\.start>.*?</day\\.start>|<type>.*?</type>|<time\\.stamp\\s*/>|<name\\.id>[^<]*</name\\.id>|<electorate>[^<]*</electorate>|<party>.*?</party>|<role\\s*/>|<in\\.gov>[^<]*</in\\.gov>|<first\\.speech>[^<]*</first\\.speech>|<name\\s+role=\"metadata\">.*?</name>|<parliament\\.no>.*?</parliament\\.no>|<session\\.no>.*?</session\\.no>|<period\\.no>.*?</period\\.no>|<chamber>.*?</chamber>)', line):\n",
    "            modified_content.append(line)\n",
    "    # Remove empty lines\n",
    "    content = re.sub(r'^\\s*$\\n', '', content, flags=re.MULTILINE)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(modified_content))\n",
    "\n",
    "def remove_inline_tags(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    # Remove <inline> tags and their attributes but keep the content inside them\n",
    "    content = re.sub(r'<inline[^>]*>', '', content)\n",
    "    content = content.replace('</inline>', '')\n",
    "    # Remove <name role=\"display\"> tags but keep the content inside them\n",
    "    content = re.sub(r'<name role=\"display\">(.*?)</name>', r'\\1', content)\n",
    "    # Remove empty lines\n",
    "    content = re.sub(r'^\\s*$\\n', '', content, flags=re.MULTILINE)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def remove_all_remaining_tags(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    # Remove all XML/HTML-like tags but keep <page> and </page> tags and their content\n",
    "    def keep_page_tags(match):\n",
    "        if match.group(0) in [\"<page>\", \"</page>\"]:\n",
    "            return match.group(0)\n",
    "        return \"\"\n",
    "    content = re.sub(r'<[^>]+>', keep_page_tags, content)\n",
    "    content = re.sub(r'^\\s*$\\n', '', content, flags=re.MULTILINE)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Iterate over directories and process each .txt file\n",
    "for directory in directories:\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                clean_and_deduplicate_pages(file_path)  # Clean and deduplicate pages\n",
    "                remove_tags(file_path)  # Remove unwanted tags\n",
    "                remove_inline_tags(file_path)  # Remove <inline> and <name role=\"display\"> tags\n",
    "                remove_all_remaining_tags(file_path)  # New step to remove all remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a url and append it to the top of each page, for citation purposes\n",
    "\n",
    "def construct_url_from_filename(filename):\n",
    "    base_url = \"https://api.parliament.uk/historic-hansard/sittings/\"\n",
    "    match = re.search(r'(\\d+)-.*?(\\d{1,2})(?:st|nd|rd|th) ([A-Za-z]+), (\\d{4})\\.txt$', filename)\n",
    "    if match:\n",
    "        day, month, year = match.group(2), match.group(3).lower(), match.group(4)\n",
    "        month_abbreviations = {\n",
    "            \"january\": \"jan\", \"february\": \"feb\", \"march\": \"mar\", \"april\": \"apr\",\n",
    "            \"may\": \"may\", \"june\": \"jun\", \"july\": \"jul\", \"august\": \"aug\",\n",
    "            \"september\": \"sep\", \"october\": \"oct\", \"november\": \"nov\", \"december\": \"dec\"\n",
    "        }\n",
    "        month_abbr = month_abbreviations.get(month, \"\")\n",
    "        url = f\"{base_url}{year}/{month_abbr}/{day}\"\n",
    "        return f\"<url>{url}</url>\\n\"\n",
    "    return \"\"\n",
    "\n",
    "for root, dirs, files in os.walk(output_directory_base):\n",
    "    if root == output_directory_base:\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                url_string = construct_url_from_filename(file)\n",
    "                f.seek(0, 0)\n",
    "                f.write(url_string + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "import os\n",
    "\n",
    "# Define the root input and output directories\n",
    "input_root_directory = 'txt/hofcoms'\n",
    "output_root_directory = 'pdf/hofcom'\n",
    "\n",
    "# Ensure the output root directory exists\n",
    "os.makedirs(output_root_directory, exist_ok=True)\n",
    "\n",
    "def convert_text_to_pdf(input_root_dir, output_root_dir):\n",
    "    for root, dirs, files in os.walk(input_root_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                try:\n",
    "                    input_file_path = os.path.join(root, filename)\n",
    "                    relative_path = os.path.relpath(root, input_root_dir)\n",
    "                    output_dir = os.path.join(output_root_dir, relative_path)\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    output_file_path = os.path.join(output_dir, filename.replace('.txt', '.pdf'))\n",
    "                    \n",
    "                    # Initialize the PDF\n",
    "                    c = canvas.Canvas(output_file_path, pagesize=letter)\n",
    "                    width, height = letter\n",
    "                    margin = 72  # 1 inch\n",
    "                    current_height = height - margin\n",
    "                    line_height = 14  # Adjust as needed\n",
    "                    \n",
    "                    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        \n",
    "                        # Process the header line (URL)\n",
    "                        header = lines[0].strip()\n",
    "                        c.setFont(\"Helvetica-Bold\", 12)\n",
    "                        c.drawString(margin, current_height, header)\n",
    "                        current_height -= line_height * 2  # Extra space after the header\n",
    "                        \n",
    "                        # Reset font for the body text\n",
    "                        c.setFont(\"Helvetica\", 10)\n",
    "                        \n",
    "                        # Process the remainder of the document\n",
    "                        for line in lines[1:]:\n",
    "                            if current_height <= margin + line_height:  # Check if we need a new page\n",
    "                                c.showPage()\n",
    "                                current_height = height - margin\n",
    "                            c.drawString(margin, current_height, line.strip())\n",
    "                            current_height -= line_height\n",
    "                    \n",
    "                    c.save()\n",
    "                    print(f\"Converted {input_file_path} to {output_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {filename} due to error: {e}\")\n",
    "\n",
    "convert_text_to_pdf(input_root_directory, output_root_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
